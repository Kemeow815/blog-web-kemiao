---
title: MS-AMP
date: 2023-11-02T12:17:40+08:00
draft: False
featuredImage: https://images.unsplash.com/photo-1693899121789-da923e57c419?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE2OTg4OTg1MTN8&ixlib=rb-4.0.3
featuredImagePreview: https://images.unsplash.com/photo-1693899121789-da923e57c419?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE2OTg4OTg1MTN8&ixlib=rb-4.0.3
---

# [Azure/MS-AMP](https://github.com/Azure/MS-AMP)

# MS-AMP: Microsoft Automatic Mixed Precision

__MS-AMP__ is an automatic mixed precision package for deep learning developed by Microsoft.

ðŸ“¢ [v0.2.0](https://github.com/Azure/MS-AMP/releases/tag/v0.2.0) has been released!

## _Check [aka.ms/msamp/doc](https://aka.ms/msamp/doc) for more details._

## Publication

<details>
<summary>
<a href="https://arxiv.org/pdf/2310.18313.pdf">FP8-LM: Training FP8 Large Language Models</a> [<b>bib</b>]
</summary>

```bibtex
@misc{fp8lm,
      title={FP8-LM: Training FP8 Large Language Models},
      author={Houwen Peng and Kan Wu and Yixuan Wei and Guoshuai Zhao and Yuxiang Yang and Ze Liu and Yifan Xiong and Ziyue Yang and Bolin Ni and Jingcheng Hu and Ruihang Li and Miaosen Zhang and Chen Li and Jia Ning and Ruizhe Wang and Zheng Zhang and Shuguang Liu and Joe Chau and Han Hu and Peng Cheng},
      year={2023},
      eprint={2310.18313},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

</details>

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
