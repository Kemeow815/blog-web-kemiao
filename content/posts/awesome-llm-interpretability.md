---
title: awesome-llm-interpretability
date: 2023-12-27T12:16:58+08:00
draft: False
featuredImage: https://images.unsplash.com/photo-1701433507837-563082c84e7e?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE3MDM2NTA1MjZ8&ixlib=rb-4.0.3
featuredImagePreview: https://images.unsplash.com/photo-1701433507837-563082c84e7e?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE3MDM2NTA1MjZ8&ixlib=rb-4.0.3
---

# [JShollaj/awesome-llm-interpretability](https://github.com/JShollaj/awesome-llm-interpretability)


# Awesome LLM Interpretability ![](https://github.com/your-username/awesome-llm-interpretability/workflows/Awesome%20Bot/badge.svg)

A curated list of amazingly awesome tools, papers, articles, and communities focused on Large Language Model (LLM) Interpretability.


## Table of Contents
- [Awesome LLM Interpretability](#awesome-llm-interpretability)
    - [LLM Interpretability Tools](#llm-interpretability-tools)
    - [LLM Interpretability Papers](#llm-interpretability-papers)
    - [LLM Interpretability Articles](#llm-interpretability-articles)
    - [LLM Interpretability Groups](#llm-interpretability-groups)

### LLM Interpretability Tools
*Tools and libraries for LLM interpretability and analysis.*

1. [Comgra](https://github.com/FlorianDietz/comgra) - Comgra helps you analyze and debug neural networks in pytorch.
2. [Pythia](https://github.com/EleutherAI/pythia) - Interpretability analysis to understand how knowledge develops and evolves during training in autoregressive transformers.
3. [Phoenix](https://github.com/Arize-ai/phoenix) - AI Observability & Evaluation - Evaluate, troubleshoot, and fine tune your LLM, CV, and NLP models in a notebook.
4. [Automated Interpretability](https://github.com/openai/automated-interpretability) - Code for automatically generating, simulating, and scoring explanations of neuron behavior.
5. [Fmr.ai](https://github.com/BizarreCake/fmr.ai) - AI interpretability and explainability platform.
6. [Attention Analysis](https://github.com/clarkkev/attention-analysis) - Analyzing attention maps from BERT transformer.
7. [SpellGPT](https://github.com/mwatkins1970/SpellGPT) - Explores GPT-3’s ability to spell own token strings.
8. [SuperICL](https://github.com/JetRunner/SuperICL) - Super In-Context Learning code which allows black-box LLMs to work with locally fine-tuned smaller models.
9. [Git Re-Basin](https://github.com/samuela/git-re-basin) - Code release for "Git Re-Basin: Merging Models modulo Permutation Symmetries.”
10. [Functionary](https://github.com/MeetKai/functionary) - Chat language model that can interpret and execute functions/plugins.
11. [Sparse Autoencoder](https://github.com/ai-safety-foundation/sparse_autoencoder) - Sparse Autoencoder for Mechanistic Interpretability.
12. [Rome](https://github.com/kmeng01/rome) - Locating and editing factual associations in GPT.
13. [Inseq](https://github.com/inseq-team/inseq) - Interpretability for sequence generation models.
14. [Neuron Viewer](https://openaipublic.blob.core.windows.net/neuron-explainer/neuron-viewer/index.html) - Tool for viewing neuron activations and explanations.
15. [LLM Visualization](https://bbycroft.net/llm) - Visualizing LLMs in low level.
16. [Vanna](https://github.com/vanna-ai/vanna) - Abstractions to use RAG to generate SQL with any LLM

---

### LLM Interpretability Papers
*Academic and industry papers on LLM interpretability.*


1. [Finding Neurons in a Haystack: Case Studies with Sparse Probing](https://arxiv.org/abs/2305.01610) - Explores the representation of high-level human-interpretable features within neuron activations of large language models (LLMs).
2. [Copy Suppression: Comprehensively Understanding an Attention Head](https://arxiv.org/abs/2310.04625) - Investigates a specific attention head in GPT-2 Small, revealing its primary role in copy suppression.
3. [Linear Representations of Sentiment in Large Language Models](https://arxiv.org/abs/2310.15154) - Shows how sentiment is represented in Large Language Models (LLMs), finding that sentiment is linearly represented in these models.
4. [Emergent world representations: Exploring a sequence model trained on a synthetic task](https://arxiv.org/abs/2210.13382) - Explores emergent internal representations in a GPT variant trained to predict legal moves in the board game Othello.
5. [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/abs/2304.14997) - Introduces the Automatic Circuit Discovery (ACDC) algorithm for identifying important units in neural networks.
6. [A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations](https://arxiv.org/abs/2302.03025) - Examines small neural networks to understand how they learn group compositions, using representation theory.
7. [Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias](https://arxiv.org/abs/2004.12265) - Causal mediation analysis as a method for interpreting neural models in natural language processing.
8. [The Quantization Model of Neural Scaling](https://arxiv.org/abs/2303.13506) - Proposes the Quantization Model for explaining neural scaling laws in neural networks.
9. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) - Presents a method for extracting accurate answers to yes-no questions from language models' internal activations without supervision.
10. [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](https://arxiv.org/abs/2305.00586) - Analyzes mathematical capabilities of GPT-2 Small, focusing on its ability to perform the 'greater-than' operation.
11. [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html) - Using a sparse autoencoder to decompose the activations of a one-layer transformer into interpretable, monosemantic features.
12. [Language models can explain neurons in language models](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) - Explores how language models like GPT-4 can be used to explain the functioning of neurons within similar models.
13. [Emergent Linear Representations in World Models of Self-Supervised Sequence Models](https://arxiv.org/abs/2309.00941) - Linear representations in a world model of Othello-playing sequence models.
14. ["Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model"](https://openreview.net/forum?id=VJEcAnFPqC) - Explores stepwise inference in autoregressive language models using a synthetic task based on navigating directed acyclic graphs.
15. ["Successor Heads: Recurring, Interpretable Attention Heads In The Wild"](https://openreview.net/forum?id=kvcbV8KQsi) - Introduces 'successor heads,' attention heads that increment tokens with a natural ordering, such as numbers and days, in LLM’s.
16. ["Large Language Models Are Not Robust Multiple Choice Selectors"](https://openreview.net/forum?id=shr9PXz7T0) - Analyzes the bias and robustness of LLMs in multiple-choice questions, revealing their vulnerability to option position changes due to inherent "selection bias”.
17. ["Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory"](https://openreview.net/forum?id=4bSQ3lsfEV) - Presents a novel approach to understanding neural networks by examining feature complexity through category theory.
18. ["Let's Verify Step by Step"](https://openreview.net/forum?id=v8L0pN6EOi) - Focuses on improving the reliability of LLMs in multi-step reasoning tasks using step-level human feedback.
19. ["Interpretability Illusions in the Generalization of Simplified Models"](https://openreview.net/forum?id=v675Iyu0ta) - Examines the limitations of simplified representations (like SVD) used to interpret deep learning systems, especially in out-of-distribution scenarios.
20. ["The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models"](https://openreview.net/forum?id=SQGUDc9tC8) - Presents a novel approach for identifying and mitigating social biases in language models, introducing the concept of 'Social Bias Neurons'.
21. ["Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition"](https://openreview.net/forum?id=VpCqrMMGVm) - Investigates how LLMs perform the task of mathematical addition.
22. ["Measuring Feature Sparsity in Language Models"](https://openreview.net/forum?id=SznHfMwmjG) - Develops metrics to evaluate the success of sparse coding techniques in language model activations.
23. [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) - Investigates how models represent more features than dimensions, especially when features are sparse.
24. [Spine: Sparse interpretable neural embeddings](https://arxiv.org/abs/1711.08792) - Presents SPINE, a method transforming dense word embeddings into sparse, interpretable ones using denoising autoencoders.
25. [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors](https://arxiv.org/abs/2103.15949) - Introduces a novel method for visualizing transformer networks using dictionary learning.
26. [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2310.11207) - Introduces Pythia, a toolset designed for analyzing the training and scaling behaviors of LLMs.
27. [On Interpretability and Feature Representations: An Analysis of the Sentiment Neuron](https://zuva.ai/science/interpretability-and-feature-representations/) - Critically examines the effectiveness of the "Sentiment Neuron”.
28. [Engineering monosemanticity in toy models](https://arxiv.org/abs/2211.09169) - Explores engineering monosemanticity in neural networks, where individual neurons correspond to distinct features.
29. [Polysemanticity and capacity in neural networks](https://arxiv.org/abs/2210.01892) - Investigates polysemanticity in neural networks, where individual neurons represent multiple features.
30. [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/) - A comprehensive exploration of the initial five layers of the InceptionV1 neural network, focusing on early vision.
31. [Visualizing and measuring the geometry of BERT](https://arxiv.org/abs/1906.02715) - Delves into BERT's internal representation of linguistic information, focusing on both syntactic and semantic aspects.
32. [Neurons in Large Language Models: Dead, N-gram, Positional](https://arxiv.org/abs/2309.04827) - An analysis of neurons in large language models, focusing on the OPT family.
33. [Can Large Language Models Explain Themselves?](https://arxiv.org/abs/2310.11207) - Evaluates the effectiveness of self-explanations generated by LLMs in sentiment analysis tasks.
34. [Interpretability in the Wild: GPT-2 small (arXiv)](https://arxiv.org/abs/2211.00593) - Provides a mechanistic explanation of how GPT-2 small performs indirect object identification (IOI) in natural language processing.
35. [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/abs/2310.11207) - Explores the use of sparse autoencoders to extract more interpretable and less polysemantic features from LLMs.
36. [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2310.11207) - Investigates the use of sparse autoencoders for enhancing the interpretability of features in LLMs.
37. [Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars](https://arxiv.org/abs/2312.01429) - Demonstrates that focusing only on specific parts like attention heads or weight matrices in Transformers can lead to misleading interpretability claims.
38. [The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets](https://arxiv.org/abs/2310.06824) - This paper investigates the representation of truth in Large Language Models (LLMs) using true/false datasets.
39. [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://arxiv.org/abs/2305.08809) - This study presents Boundless Distributed Alignment Search (Boundless DAS), an advanced method for interpreting LLMs like Alpaca.
40. [Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405) - Introduces Representation Engineering (RepE), a novel approach for enhancing AI transparency, focusing on high-level representations rather than neurons or circuits.
41. [Explaining black box text modules in natural language with language models](https://arxiv.org/abs/2305.09863) - Natural language explanations for LLM attention heads, evaluated using synthetic text
42. [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models](https://arxiv.org/abs/2304.12918) - Explain each LLM neuron as a graph
43. [Augmenting Interpretable Models with LLMs during Training](https://arxiv.org/abs/2209.11799) - Use LLMs to build interpretable classifiers of text data

---

### LLM Interpretability Articles
*Insightful articles and blog posts on LLM interpretability.*

1. [A New Approach to Computation Reimagines Artificial Intelligenceg](https://www.quantamagazine.org/a-new-approach-to-computation-reimagines-artificial-intelligence-20230413/?mc_cid=ad9a93c472&mc_eid=506130a407) - Discusses hyperdimensional computing, a novel method involving hyperdimensional vectors (hypervectors) for more efficient, transparent, and robust artificial intelligence.
2. [Interpreting GPT: the logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - Explores how the logit lens, reveals a gradual convergence of GPT's probabilistic predictions across its layers, from initial nonsensical or shallow guesses to more refined predictions.
3. [A Mechanistic Interpretability Analysis of Grokking](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking) - Explores the phenomenon of 'grokking' in deep learning, where models suddenly shift from memorization to generalization during training.
4. [200 Concrete Open Problems in Mechanistic Interpretability](https://www.lesswrong.com/s/yivyHaCAmMJ3CqSyj) - Series of posts discussing open research problems in the field of Mechanistic Interpretability (MI), which focuses on reverse-engineering neural networks.
5. [Evaluating LLMs is a minefield](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/) - Challenges in assessing the performance and biases of large language models (LLMs) like GPT.
6. [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) - Method that uses gradients for a linear approximation of activation patching in neural networks.
7. [Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) - Introduces causal scrubbing, a method for evaluating the quality of mechanistic interpretations in neural networks.
8. [A circuit for Python docstrings in a 4-layer attention-only transformer](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only) - Proposes the Quantization Model for explaining neural scaling laws in neural networks.
9. [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) - Examines a specific neural circuit within a 4-layer transformer model responsible for generating Python docstrings.
10. [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://arxiv.org/abs/2207.13243) - Survey on mechanistic interpretability

---

### LLM Interpretability Groups
*Communities and groups dedicated to LLM interpretability.*

1. [Alignment Lab AI](https://discord.com/invite/ad27GQgc7K) - Group of researchers focusing on AI alignment.
8. [Nous Research](https://discord.com/invite/AVB9jkHZ) - Research group discussing various topics on interpretability.
9. [EleutherAI](https://discord.com/invite/4pEBpVTN) - Non-profit AI research lab that focuses on interpretability and alignment of large models. 

---

## Contributing and Collaborating
Please see [CONTRIBUTING](https://github.com/JShollaj/awesome-llm-interpretability/blob/master/CONTRIBUTING.md) and [CODE-OF-CONDUCT](https://github.com/JShollaj/awesome-llm-interpretability/blob/master/CODE-OF-CONDUCT.md) for details.

